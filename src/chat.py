from langchain.chat_models import init_chat_model
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.chat_history import InMemoryChatMessageHistory
from langchain_core.runnables import RunnableWithMessageHistory
from search import Search
from dotenv import load_dotenv

load_dotenv()

def main():
    """Fun√ß√£o principal do chat"""
    print("ü§ñ Chat sobre Teste de Performance")
    print("=" * 40)
    print("Digite 'sair' para encerrar o chat")
    print("=" * 40)
    
    try:
        
        print()
        
        # Loop principal do chat
        while True:
            try:
                # Receber input do usu√°rio
                user_question = input("Fa√ßa uma pergunta: ").strip()
                
                # Verificar se quer sair
                if user_question.lower() in ['sair', 'exit', 'quit', 'q']:
                    print("üëã At√© logo!")
                    break
                
                # Verificar se a entrada n√£o est√° vazia
                if not user_question:
                    print("‚ùå Por favor, digite uma pergunta.")
                    continue
                
                # Gerar resposta usando LangChain
                context = (Search.search_query(user_question))
                system = ("""
                    CONTEXTO:{context}
                            
                    REGRAS:
                    - Responda somente com base no CONTEXTO.
                    - Se a informa√ß√£o n√£o estiver explicitamente no CONTEXTO, responda:
                    "N√£o tenho informa√ß√µes necess√°rias para responder sua pergunta."
                    - Nunca invente ou use conhecimento externo.
                    - Nunca produza opini√µes ou interpreta√ß√µes al√©m do que est√° escrito.

                    EXEMPLOS DE PERGUNTAS FORA DO CONTEXTO:
                    Pergunta: "Qual √© a capital da Fran√ßa?"
                    Resposta: "N√£o tenho informa√ß√µes necess√°rias para responder sua pergunta."

                    Pergunta: "Quantos clientes temos em 2024?"
                    Resposta: "N√£o tenho informa√ß√µes necess√°rias para responder sua pergunta."

                    Pergunta: "Voc√™ acha isso bom ou ruim?"
                    Resposta: "N√£o tenho informa√ß√µes necess√°rias para responder sua pergunta."

                    PERGUNTA DO USU√ÅRIO: {question}

                    RESPONDA A {question}
                            
                    """)
                user = ("user", "{question}")
                chat_prompt = ChatPromptTemplate([system, user])
                messages = chat_prompt.format_messages(context=context, question=user_question)
                model = init_chat_model(model="gemini-2.5-flash", model_provider="google_genai", temperature=0.5)
                result = model.invoke(messages)
                print()
                print("Resposta: ", result.content)
                print()
                
            except KeyboardInterrupt:
                print("\nüëã Chat interrompido pelo usu√°rio. At√© logo!")
                break
            except Exception as e:
                print(f"‚ùå Erro: {e}")
                print("Tente novamente ou digite 'sair' para encerrar.")
                print()
                
    except Exception as e:
        print(f"‚ùå Erro ao inicializar o chat: {e}")
        print("Verifique sua conex√£o com a internet e a validade da API key.")

if __name__ == "__main__":
    main()






"""

prompt = ChatPromptTemplate.from_messages([
    ("system", system),
    MessagesPlaceholder(variable_name="history"), #hist√≥rico de mensagens
    ("human", "{user_question}")
])

chat_model = init_chat_model(model="gemini-2.5-flash", model_provider="google_genai")

chain = context | prompt | chat_model

session_store: dict[str, InMemoryChatMessageHistory] = {}

# Pega o hist√≥tico da sessao
def get_session_history(session_id: str) -> InMemoryChatMessageHistory:
    if session_id not in session_store:
        session_store[session_id] = InMemoryChatMessageHistory()
    return session_store[session_id]

conversational_chain = RunnableWithMessageHistory(
    chain,
    get_session_history,
    input_messages_key="input",
    history_messages_key="history",
)

# simular criacao de novas sessoes
config = {"configurable": {"session_id": "demo-session"}}

# Interactions
response1 = conversational_chain.invoke({"input": input("Fa√ßa uma pergunta sobre teste de performance: ").strip()}, config=config)
print("Assistant: ", response1.content)
print("-"*30)


"""